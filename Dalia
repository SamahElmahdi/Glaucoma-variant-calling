#Activate the NGS1 enviroment
conda activate ngs1
#Create a new directory for the project
mkdir glucoma
#Download & install SRA tools according to NCBI guide (SRA tools are required to extract a fraction of the reads to work on)
wget "ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-centos_linux64.tar.gz"
tar -xzf sratoolkit.current-centos_linux64.tar.gz
#Install sra tool kit according to the manual
./fastq-dump
sudo apt sudo apt sratoolkit
#Download 8 million reads for PCAG control "SRR5858161" using the following command:
fastq-dump -I --split-files -X 8000000 SRR5858161
#Install the software
conda install -c bioconda fastqc 
conda install -c bioconda multiqc
Run fastqc on the downloaded 8 million reads in fastq format
for f in ~/glucoma/*.fastq;do fastqc -t 1 -f fastq $f;done
#create a new directory for the chromosomes
mkdir chr && cd chr
#Download the reference chromosome "chromosome 20"
wget ftp://ftp.ensembl.org/pub/release-99/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.20.fa.gz
#Download chromosome 11
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr11.fa.gz
#Download chromosome 6
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr6.fa.gz 
#Download chromosome 3
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr3.fa.gz
#combine the 4 fasta files in one fasta file
zcat *.fa.gz > combined_chr.fa.gz
#Unzip it
gunzip -k combined_chr.fa.gz
#make the annotation gtf file for chromosomes 3,6,11 and 20
grep "^#" Homo_sapiens.GRCh38.99.chr.gtf > Homo_sapiens.GRCh38.99.chr_3_11_6_20.gtf
grep "^3" Homo_sapiens.GRCh38.99.chr.gtf >> Homo_sapiens.GRCh38.99.chr_3_11_6_20.gtf
grep "^11" Homo_sapiens.GRCh38.99.chr.gtf >> Homo_sapiens.GRCh38.99.chr_3_11_6_20.gtf
grep "^6" Homo_sapiens.GRCh38.99.chr.gtf >> Homo_sapiens.GRCh38.99.chr_3_11_6_20.gtf
grep "^20" Homo_sapiens.GRCh38.99.chr.gtf >> Homo_sapiens.GRCh38.99.chr_3_11_6_20.gtf
##Hisat2
#indexing
ln -s ~/glucoma/chr/combined.fa .
hisat2_extract_splice_sites.py ~/glucoma/Homo_sapiens.GRCh38.99.chr_3_11_6_20.gtf > splicesites.tsv
hisat2_extract_exons.py ~/glucoma/Homo_sapiens.GRCh38.99.chr_3_11_6_20.gtf > exons.tsv
hisat2-build -p 1 --ss splicesites.tsv --exon exons.tsv combined.fa combined
#Aligning
cd ~/glucoma/hisat_align
R1="$HOME/glucoma/SRR5858161_1.fastq"
R2="$HOME/glucoma/SRR5858161_2.fastq"
hisat2 -p 1 -x hisatIndex/combined --dta --rna-strandness RF -1 $R1 -2 $R2 -S gluc_Rep1.sam
#************** Hisat2 kept terminating due to the huge file sizes, so we will use chromosome 20 only as an index ***************
#indexing
ln -s ~/glucoma/chr/Homo_sapiens.GRCh38.dna.chromosome.20.fa .
hisat2_extract_splice_sites.py ~/glucoma/Homo_sapiens.GRCh38.99.chr20.gtf > splicesites.tsv
hisat2_extract_exons.py ~/glucoma/Homo_sapiens.GRCh38.99.chr20.gtf > exons.tsv
hisat2-build -p 1 --ss splicesites.tsv --exon exons.tsv Homo_sapiens.GRCh38.dna.chromosome.20.fa Homo_sapiens.GRCh38.dna.chromosome.20
#Aligning
cd ~/glucoma/hisat_align
R1="$HOME/glucoma/SRR5858161_1.fastq"
R2="$HOME/glucoma/SRR5858161_2.fastq"
hisat2 -p 1 --rg ID:SRR5858161_F7_II.2 --rg SM:SRR5858161_F7_II.2 --rg PL:ILLUMINA --rg LB:SRR5858161_F7_II.2 --rg PU:SRR5858161_F7_II.2 -x ~/glucoma/Homo_sapiens.GRCh38.dna.chromosome.20 -1 $R1 -2 $R2 -S SRR5858161.sam
#Generating and sorting BAM file:
for samfile in *.sam;do
  sample=${samfile%.sam}
  samtools view -hbo $sample.bam $samfile
  samtools sort $sample.bam -o $sample.sorted.bam
done
#Mapping QC:
for bamFile in *.sorted.bam;do
  output=${bamFile%.sorted.bam}
  samtools depth $bamFile | awk '{{sum+=$3}} END {{print "Average = ",sum/NR, "No of covered Nuc = ", NR}}' > $output.cov
  samtools flagstat $bamFile > $output.stat
done
#Install Picard tools
conda install -c bioconda picard 
picard_path=$CONDA_PREFIX/share/picard-* ## 2.21.7-0
#Mark Duplicate reads:
java  -Xmx2g -jar $picard_path/picard.jar MarkDuplicates INPUT=SRR5858161 OUTPUT=SRR5858161.dedup.bam METRICS_FILE=SRR5858161.metrics.txt;
done
#Install GATK4
conda install -c bioconda gatk4 
#Indexing:
#samples
java -Xmx2g -jar $picard_path/picard.jar BuildBamIndex VALIDATION_STRINGENCY=LENIENT INPUT=SRR5858161.dedup.bam
#Reference
ln -s ~/glucoma/Homo_sapiens.GRCh38.dna.chromosome.20.fa
java -Xmx2g -jar $picard_path/picard.jar CreateSequenceDictionary R=Homo_sapiens.GRCh38.dna.chromosome.20.fa O=Homo_sapiens.GRCh38.dna.chromosome.20.dict
samtools faidx Homo_sapiens.GRCh38.dna.chromosome.20.fa
#Download known varinats
#Download known polymorphic sites
wget ftp://ftp.ensembl.org/pub/release-99/variation/vcf/homo_sapiens/homo_sapiens-chr20.vcf.gz
gunzip -k homo_sapiens-chr20.vcf.gz
#Indexing the known variants file:
gatk IndexFeatureFile -I homo_sapiens-chr20.vcf
##The tutorial contains a step for selecting known variants on a certain chromosome, we do not need to run this step as our downloaded file contains knwon variants on chromosome 20 only.
#Recalibrate bases BQSR
#BaseRecalibrator Tool: builds a model of covariation based on the data and a set of known variants.
gatk --java-options "-Xmx2G" BaseRecalibrator \
-R Homo_sapiens.GRCh38.dna.chromosome.20.fa -I SRR5858161.dedup.bam --known-sites homo_sapiens-chr20.vcf \
-O SRR5858161.report
#Apply BQSR Tool: adjusts the base quality scores in the data based on the model.
gatk --java-options "-Xmx2G" ApplyBQSR \
-R Homo_sapiens.GRCh38.dna.chromosome.20.fa -I SRR5858161.dedup.bam -bqsr SRR5858161.report \
-O SRR5858161.bqsr.bam --add-output-sam-program-record --emit-original-quals
#Joint variant calling using HaplotypeCaller:
## assess genotype likelihood per-sample
gatk --java-options "-Xmx2G" HaplotypeCaller \
  -R Homo_sapiens.GRCh38.dna.chromosome.20.fa -I SRR5858161.bqsr.bam \
  --emit-ref-confidence GVCF \
  --pcr-indel-model NONE \
  -O SRR5858161.gvcf

